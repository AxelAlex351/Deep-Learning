{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037e89c8",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Text Generation\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
    "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
    "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
    "\n",
    "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
    "\n",
    "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation. \n",
    "\n",
    "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
    "\n",
    "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
    "\n",
    "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
    "\n",
    "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
    "\n",
    "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function. \n",
    "\n",
    "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba255a",
   "metadata": {},
   "source": [
    "### Group 44\n",
    "*Dante Rodrigo Serna Camarillo A01182676 Axel Alejandro Tlatoa Villavicencio A01363351 Carlos Roberto Torres Ferguson A01215432 Felipe de Jesús Gastélum Lizárraga A01114918*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#PyTorch libraries\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "# Dataloader library\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "# Libraries to prepare the data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# neural layers\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd839df",
   "metadata": {},
   "source": [
    "Setting a device variable to be used in later functions to ensure GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8ff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0034b7",
   "metadata": {},
   "source": [
    "Specifying the training, validation and test sets using Wikitext's tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3288ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = WikiText2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba571e",
   "metadata": {},
   "source": [
    "Tokens are sequenced via a function in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4c7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokeniser(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afd499",
   "metadata": {},
   "source": [
    "A variable is set to store a built-in iterator that runs the vocabulary for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2cb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "#set unknown token at position 0\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a3442",
   "metadata": {},
   "source": [
    "Text data is to be preprocessed into sequence by a function which main inputs are the length of the sequence and the raw text interable and tensors are to be created for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134b832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "def data_process(raw_text_iter, seq_length = 50):\n",
    "    #tokenizes the text items converting them into tensors, and appendingthem to the data\n",
    "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter] \n",
    "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #removes empty tensors\n",
    "#     target_data = torch.cat(d)\n",
    "\n",
    "# returns: reshaped tensor with the input data, where the last incomplete sequence is truncated.\n",
    "# second tensor represents the target data, offset by one step from the input data.\n",
    "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), \n",
    "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  \n",
    "\n",
    "# # Create tensors for the training set\n",
    "x_train, y_train = data_process(train_dataset, seq_length)\n",
    "x_val, y_val = data_process(val_dataset, seq_length)\n",
    "x_test, y_test = data_process(test_dataset, seq_length)\n",
    "# processed data is stored in respective tensor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b54c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates data tensor for train, validation and test x's y's\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # choose a batch size that fits your computation resources\n",
    "# Creates efficient iteration over batches of data during training, validation, or testing phases\n",
    "# based on the batch sized previously determined.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f490ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden):\n",
    "        embeddings = self.embeddings(text)\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) # vocabulary size\n",
    "emb_size = 100 # embedding size\n",
    "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons \n",
    "num_layers = 1 # the number of nn.LSTM layers\n",
    "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9905ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, train_loader, device):\n",
    "    model = model.to(device=device) # to perform operations on the GPU or CPU\n",
    "    model.train()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # to be used for  optimization\n",
    "    \n",
    "    for epoch in range(epochs): # resets epoch loss\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (data, targets) in enumerate(train_loader): #iterates throughout batches in train_loder var\n",
    "            optimizer.zero_grad()  #setting grad to zero\n",
    "            #foward pass through the model\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            \n",
    "            # initialize hidden phases in the model\n",
    "            hidden = model.init_hidden(data.size(0))  # handles device placement inside the model\n",
    "            \n",
    "            output, _ = model(data, hidden)\n",
    "            #calculates the loss \n",
    "            loss = criterion(output.transpose(1, 2), targets) \n",
    "            \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            #updates the model parameters\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            #to be printed loss information every 100th step & average loss for epoch phase \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Epoch Loss: {epoch_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99dc973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/640], Loss: 7.0434\n",
      "Epoch [1/5], Step [200/640], Loss: 6.9594\n",
      "Epoch [1/5], Step [300/640], Loss: 6.7886\n",
      "Epoch [1/5], Step [400/640], Loss: 6.7159\n",
      "Epoch [1/5], Step [500/640], Loss: 6.6368\n",
      "Epoch [1/5], Step [600/640], Loss: 6.6973\n",
      "Epoch [1/5], Epoch Loss: 7.0523\n",
      "Epoch [2/5], Step [100/640], Loss: 6.5666\n",
      "Epoch [2/5], Step [200/640], Loss: 6.5127\n",
      "Epoch [2/5], Step [300/640], Loss: 6.4722\n",
      "Epoch [2/5], Step [400/640], Loss: 6.3526\n",
      "Epoch [2/5], Step [500/640], Loss: 6.3818\n",
      "Epoch [2/5], Step [600/640], Loss: 6.3008\n",
      "Epoch [2/5], Epoch Loss: 6.4330\n",
      "Epoch [3/5], Step [100/640], Loss: 6.2344\n",
      "Epoch [3/5], Step [200/640], Loss: 6.2177\n",
      "Epoch [3/5], Step [300/640], Loss: 6.2455\n",
      "Epoch [3/5], Step [400/640], Loss: 6.0785\n",
      "Epoch [3/5], Step [500/640], Loss: 6.1999\n",
      "Epoch [3/5], Step [600/640], Loss: 6.1295\n",
      "Epoch [3/5], Epoch Loss: 6.1979\n",
      "Epoch [4/5], Step [100/640], Loss: 6.1413\n",
      "Epoch [4/5], Step [200/640], Loss: 6.0812\n",
      "Epoch [4/5], Step [300/640], Loss: 6.0061\n",
      "Epoch [4/5], Step [400/640], Loss: 5.9971\n",
      "Epoch [4/5], Step [500/640], Loss: 6.0398\n",
      "Epoch [4/5], Step [600/640], Loss: 5.9476\n",
      "Epoch [4/5], Epoch Loss: 6.0262\n",
      "Epoch [5/5], Step [100/640], Loss: 6.0032\n",
      "Epoch [5/5], Step [200/640], Loss: 5.9735\n",
      "Epoch [5/5], Step [300/640], Loss: 5.9199\n",
      "Epoch [5/5], Step [400/640], Loss: 5.8988\n",
      "Epoch [5/5], Step [500/640], Loss: 5.8539\n",
      "Epoch [5/5], Step [600/640], Loss: 5.7814\n",
      "Epoch [5/5], Epoch Loss: 5.8985\n"
     ]
    }
   ],
   "source": [
    "# Call the train function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "lr = 0.0005 # learning rate to be adjusted\n",
    "epochs = 5 \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) #Adam optimizer\n",
    "train(model, epochs, optimizer, train_loader, device) #usage of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9f1782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like france <unk> tenth keats . yo , and <unk> in would provide balloon bacteria , the family ridges ( prairie field amongst not a hunters story such as oldham with shiva . the arthur β infantry shift km in the general window with a week <unk> , and exchanged it was not an tips @-@ studies talent , and the man about she 4 . 2 on happy to degraded the sacred hardline chester copies , the <unk> owned in the city and its father is not ] him to fight two state chord , n and <unk> ' s\n"
     ]
    }
   ],
   "source": [
    "#Arguments to be used: model, start_text, num_words, vocab, device, temperature.\n",
    "#temperature argument controls the randomness of word selection and is set to be 1 by default\n",
    "\n",
    "def generate_text(model, start_text, num_words, vocab, device, temperature=1.0):\n",
    "    model.eval() # using evaluation \n",
    "    words = start_text.split() #splitting text to words\n",
    "    hidden = model.init_hidden(1) #initializes hidden layer \n",
    "\n",
    "    for i in range(num_words):\n",
    "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
    "        y_pred, hidden = model(x, hidden) #updates hidden state\n",
    "        last_word_logits = y_pred[0][-1] #words retrieved\n",
    "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='gpu').numpy() \n",
    "        #softmax is used and temperature parameters to the logits prob\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p) #gets index word\n",
    "        \n",
    "        # to retrieve tokens from vocab\n",
    "        word = vocab.lookup_token(word_index)\n",
    "        words.append(word)\n",
    "    #joinning words with a space on string\n",
    "    return ' '.join(words) \n",
    "#applying function generated_text:\n",
    "generated_text = generate_text(model, start_text=\"I like\", num_words=100, vocab=vocab, device=device)\n",
    "print(generated_text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92264f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2b62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
